{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Functions\n",
    "def tan_hyp(y) :\n",
    "    return np.tanh(y)\n",
    "\n",
    "def softmax(y) :\n",
    "    return np.exp(y-max(y))/np.sum(np.exp(y-max(y)))\n",
    "\n",
    "def linear(y) : \n",
    "    return y\n",
    "\n",
    "#Gradients\n",
    "\n",
    "def der_softmax(y) : \n",
    "    a = np.zeros((len(y),len(y)))\n",
    "    np.fill_diagonal(a,y)\n",
    "    y = np.reshape(y,(len(y),-1))\n",
    "    b = np.dot(y,np.transpose(y))\n",
    "    return a - b\n",
    "\n",
    "def der_tan_hyp(y) : \n",
    "    return (1-y)*(1+y)\n",
    "\n",
    "def der_linear(y) :\n",
    "    a = np.zeros((len(y),len(y)))\n",
    "    np.fill_diagonal(a,y)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss functions\n",
    "def mse(y,t) : \n",
    "    return 0.5*np.sum((y-t)**2)\n",
    "\n",
    "def cross_entropy(y,t):\n",
    "    return -np.sum(t*np.log(y))\n",
    "\n",
    "def grad_cross_entropy(y,t) :\n",
    "    return -np.divide(t,y)\n",
    "\n",
    "def grad_mse(y,t) : \n",
    "    return y-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwpass_customweights(activation_functions,training_sample,w) : \n",
    "        a_list = [training_sample]\n",
    "        h_list = [training_sample]\n",
    "        output = training_sample\n",
    "        \n",
    "        for (activation_function,weight) in zip(activation_functions,w) :\n",
    "            \n",
    "            output = np.append(output,np.ones((1,1)),axis = 0)\n",
    "            output = np.dot(weight,output)\n",
    "            a_list.append(output)\n",
    "            \n",
    "            output = eval(activation_function)(output)\n",
    "            h_list.append(output)\n",
    "        return a_list , h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network :\n",
    "    \n",
    "    def __init__(self, n_hidden,list_nodes,loss_fn,list_activation_functions,learning_rate,momentum) :\n",
    "        self.n_hidden = n_hidden\n",
    "        self.list_nodes = list_nodes\n",
    "        self.loss_fn = loss_fn\n",
    "        self.list_activation_functions = list_activation_functions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.weights = []\n",
    "        self.grad_past = []\n",
    "        \n",
    "        for i in range(n_hidden+1) :\n",
    "            np.random.seed(42)\n",
    "            weight_layer = np.random.randn(list_nodes[i+1],list_nodes[i]+1)\n",
    "            grad_past_layer = np.zeros((list_nodes[i+1],list_nodes[i]+1))\n",
    "            self.weights.append(weight_layer)\n",
    "            self.grad_past.append(grad_past_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self,training_sample):\n",
    "        a_list = [training_sample]\n",
    "        h_list = [training_sample]\n",
    "        output = training_sample\n",
    "        \n",
    "        for (activation_function,weight) in zip(self.list_activation_functions,self.weights) :\n",
    "            \n",
    "            output = np.append(output,np.ones((1,1)),axis = 0)\n",
    "            output = np.dot(weight,output)\n",
    "            a_list.append(output)\n",
    "            \n",
    "            output = eval(activation_function)(output)\n",
    "            h_list.append(output)\n",
    "        return a_list , h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def calc_loss(self,calc_output,actual_output) :\n",
    "        return eval(self.loss_fn)(calc_output,actual_output)\n",
    "    \n",
    "    def backward_pass(self,h_list,intermediate_loss,actual_output) : \n",
    "        '''implementation of backpropagation'''\n",
    "        \n",
    "        der_output_act = eval('der_'+self.list_activation_functions[-1])(h_list[-1])\n",
    "        grad_lf = eval('grad_'+self.loss_fn)(h_list[-1],actual_output)\n",
    "        grad_layer = np.dot(der_output_act,grad_lf)\n",
    "                \n",
    "        for i in range(len(self.weights)) : \n",
    "            \n",
    "            grad_weight_layer = np.dot(grad_layer,np.transpose(np.append(h_list[len(h_list)-i-2],np.ones((1,1)),axis = 0)))\n",
    "            \n",
    "            grad_prev_output = np.dot(np.transpose(self.weights[len(self.weights)-i-1])[:-1,:],grad_layer)\n",
    "            grad_layer = grad_prev_output * eval('der_'+self.list_activation_functions[len(self.list_activation_functions)-i-2])(h_list[len(h_list)-i-2])\n",
    "            \n",
    "            self.weights[len(self.weights)-i-1] = self.gen_delta_rule(self.weights[len(self.weights)-i-1],grad_weight_layer,self.grad_past[len(self.grad_past)-i-1]) \n",
    "            self.grad_past[len(self.grad_past)-i-1] = self.learning_rate*grad_weight_layer\n",
    "            \n",
    "    def gen_delta_rule(self,weights_present,grad_present,weight_change_previous) :\n",
    "        return weights_present - (grad_present*self.learning_rate) - (weight_change_previous*self.momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self,input_array,output_array,valid_input_array,valid_output_array) : \n",
    "        loss_list = []\n",
    "        valid_loss_list = []\n",
    "        epoch_num = 0\n",
    "        prev_loss_avg = 0\n",
    "        loss_avg = 0\n",
    "        \n",
    "        while (epoch_num<=0 or abs(prev_loss_avg - loss_avg)>=1e-5) : \n",
    "            \n",
    "            prev_loss_avg = loss_avg\n",
    "            loss_avg = 0\n",
    "                \n",
    "            print('Epoch Number : ',epoch_num)\n",
    "            print('Started')\n",
    "            print('----------------------------------------')\n",
    "            for (training_sample,actual_output) in zip(input_array,output_array) :\n",
    "                \n",
    "                a_list,h_list = self.forward_pass(training_sample)\n",
    "                \n",
    "                intermediate_loss = self.calc_loss(h_list[-1],actual_output)\n",
    "                \n",
    "                self.backward_pass(h_list,intermediate_loss,actual_output)\n",
    "                \n",
    "                loss_avg = loss_avg + intermediate_loss\n",
    "            \n",
    "            loss_avg = loss_avg/len(input_array)\n",
    "            loss_list.append(loss_avg)\n",
    "            \n",
    "            valid_loss_avg = 0\n",
    "            \n",
    "            for (training_sample,actual_output) in zip(valid_input_array,valid_output_array) :\n",
    "                \n",
    "                a_list,h_list = self.forward_pass(training_sample)\n",
    "                \n",
    "                intermediate_loss = self.calc_loss(h_list[-1],actual_output)\n",
    "                                \n",
    "                valid_loss_avg = valid_loss_avg + intermediate_loss\n",
    "            \n",
    "            valid_loss_avg = valid_loss_avg/len(valid_input_array)\n",
    "            valid_loss_list.append(valid_loss_avg)\n",
    "            \n",
    "            print('Average Loss : ',loss_avg)    \n",
    "            print('----------------------------------------')\n",
    "            epoch_num = epoch_num + 1\n",
    "            \n",
    "            return loss_list,valid_loss_list\n",
    "            \n",
    "def test(self,input_array,output_array) : \n",
    "        accuracy = 0\n",
    "        for (sample,output) in zip(input_array,output_array) : \n",
    "            \n",
    "            a_list,h_list = self.forward_pass(sample)\n",
    "            calc_output = h_list[-1]\n",
    "            \n",
    "            if np.where(calc_output == np.amax(calc_output)) == np.where(output == np.amax(output)) :\n",
    "                accuracy = accuracy + 1\n",
    "            \n",
    "        accuracy = accuracy / len(input_array)    \n",
    "        return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "input_array = []\n",
    "output_array = []\n",
    "\n",
    "test_input_array = []\n",
    "train_input_array= []\n",
    "\n",
    "data = pd.read_csv('<enter csv file>')\n",
    "\n",
    "for i in range(len(data)) :\n",
    "    input_array.append(np.array([data['x1'][i],data['x2'][i]]).reshape(2,1))\n",
    "    if data['label'][i] == 0 :\n",
    "        output_array.append(np.array([1.0,0.0,0.0]).reshape(3,1))\n",
    "    if data['label'][i] == 1 :\n",
    "        output_array.append(np.array([0.0,1.0,0.0]).reshape(3,1))\n",
    "    if data['label'][i] == 2 :\n",
    "        output_array.append(np.array([0.0,0.0,1.0]).reshape(3,1))  \n",
    "\n",
    "test_input_array = input_array[int(np.floor(len(data)*3/4)):]\n",
    "test_output_array = output_array[int(np.floor(len(data)*3/4)):]\n",
    "\n",
    "train_input_array = input_array[:int(np.floor(len(data)*3/4))]\n",
    "train_output_array = output_array[:int(np.floor(len(data)*3/4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = neural_network(2,[2,5,5,3],'cross_entropy',['tan_hyp','tan_hyp','softmax'],0.001,0.8)\n",
    "\n",
    "list_avg_error,list_valid_avg_error = nn.train(train_input_array,train_output_array,test_input_array,test_output_array)\n",
    "\n",
    "accuracy = nn.test(test_input_array,test_output_array)\n",
    "\n",
    "print(\"Training Finished\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Accuracy : \",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
